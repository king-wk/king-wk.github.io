<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color" content="#3367D6"/>
  <link rel="apple-touch-icon" href="/icons-192.png">
  <link rel="manifest" href="/manifest.json">
  
  <meta name="generator" content="Hexo 5.4.0">

  
    <meta name="description" content=" keywords: ">
  

  

  
    <meta name="author" content="double 静">
  

  

  

  <title>Pytorch 数据操作 | 怪兽少女</title>

  

  
    <link rel="shortcut icon" href="/fig.ico">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@1.1.3/index.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlightjs@9.16.2/styles/monokai.css">
  

  
<link rel="stylesheet" href="/css/style.css">

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>
<body>
  <div class="root-container">
    
<!-- header container -->
<header class="header-container post">
  
    <div class="post-image" style="background-image: url(https://cdn.magdeleine.co/wp-content/uploads/2018/09/bloom-blooming-blossom-1303953-1400x938.jpg)"></div>
  

  <!-- navbar -->
<nav class="navbar">
  <div class="navbar-content">
    <!-- logo -->
    <div class="navbar-logo">
      <a href="/">
        
          怪兽少女
        
      </a>
    </div>
    <!-- link -->
    <div class="navbar-link">
      <div class="navbar-btn">
        <div></div>
        <div></div>
        <div></div>
      </div>
      <ul class="navbar-list">
        
          <li class="navbar-list-item"><a href="/">首页</a></li>
        
          <li class="navbar-list-item"><a href="/summary">归档</a></li>
        
          <li class="navbar-list-item"><a href="/links">分享</a></li>
        
          <li class="navbar-list-item"><a href="/about">关于</a></li>
        
      </ul>
    </div>
  </div>
</nav>

  
  

  
  

  
  

  
  

  
  
    <div class="header-content">
      <div class="post-text layout-block">
        <div class="layout-margin">
          <h1 class="title-wrap">Pytorch 数据操作</h1>
          <h2 class="title-sub-wrap">
            <strong>double 静</strong>
            <span>发布于</span>
            <time  class="article-date" datetime="2021-11-24T15:02:36.000Z" itemprop="datePublished">2021-11-24</time>
          </h2>
          <ul class="wrap-list dark">
  
    <li><a href="/categories/DeepLearning/">📒 DeepLearning</a></li>
  
</ul>
          <ul class="wrap-list dark">
  
    <li><a href="/tags/DeepLearning/">🏷️ DeepLearning</a></li>
  
    <li><a href="/tags/Pytorch/">🏷️ Pytorch</a></li>
  
</ul>
        </div>
      </div>
    </div>
  

  
  
  
</header>

    <!-- 文章 -->

<!-- 文章内容 -->
<div class="body-container">
  <article class="content-container layout-block post-container">
    <div class="article-info">
      
      
      
      
      <section class="article-entry markdown-body layout-margin content-padding--large soft-size--large soft-style--box">
        <h2 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h2><p>在 PyTorch 中，torch.Tensor 是存储和变换数据的主要工具，它的主要类型和操作如下：</p>
<h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>tensor 有三个属性：torch.dtype, torch.device 和 torch.layout</p>
<ul>
<li><p>torch.dtype 是表示 torch.Tensor 的数据类型，如下是完整的 dtype：</p>
<p>  <img src="/assets/DeepLearning/tensor.png" alt="torch.dtype"></p>
</li>
<li><p>torch.device 是一个对象，表示正在或将要分配 torch.Tensor 的设备。</p>
<p>  torch.device 包含设备类型 (“cpu” 或 “cuda”) 和设备类型的可选设备序号。如果设备序号不存在，则此对象将始终表示设备类型的当前设备，即使在调用 torch.cuda.set_device() 之后也是如此。</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>, index=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">device(<span class="built_in">type</span>=<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.device(<span class="string">&#x27;cuda&#x27;</span>)  <span class="comment"># current cuda device</span></span><br><span class="line">device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.device(<span class="string">&#x27;cuda&#x27;</span>, <span class="number">0</span>)</span><br><span class="line">device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>, index=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.device(<span class="string">&#x27;cpu&#x27;</span>, <span class="number">0</span>)</span><br><span class="line">device(<span class="built_in">type</span>=<span class="string">&#x27;cpu&#x27;</span>, index=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.device(<span class="number">1</span>)</span><br><span class="line">device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>, index=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randn((<span class="number">2</span>,<span class="number">3</span>), device=torch.device(<span class="string">&#x27;cuda:1&#x27;</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randn((<span class="number">2</span>,<span class="number">3</span>), device=<span class="string">&#x27;cuda:1&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randn((<span class="number">2</span>,<span class="number">3</span>), device=<span class="number">1</span>)  <span class="comment"># legacy</span></span><br></pre></td></tr></table></figure>
<p>  此外，cpu 和 cuda 设备的转换使用 ‘to’ 来实现：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>device_cpu = torch.device(<span class="string">&quot;cuda&quot;</span>)  <span class="comment"># 声明 cuda 设备</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>device_cuda = torch.device(<span class="string">&#x27;cuda&#x27;</span>)  <span class="comment"># 设备 cpu 设备</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = torch.Tensor([<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data.to(device_cpu)  <span class="comment"># 将数据转为 cpu 格式</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data.to(device_cuda)   <span class="comment"># 将数据转为 cuda 格式</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>torch.layout 是一个表示 torch.Tensor 内存布局的对象。 目前，我们支持 torch.strided（密集张量）并且对 torch.sparse_coo（稀疏 COO 张量）提供测试版支持。 torch.strided 代表密集张量，是最常用的内存布局。 每个跨步张量都有一个关联的 torch.Storage，用于保存其数据。 这些张量提供了存储的多维、跨步视图。 步幅是一个整数列表：第 k 步幅表示在张量的第 k 维中从一个元素到下一个元素所需的内存跳跃。 这个概念使得高效地执行许多张量操作成为可能。</p>
</li>
</ul>
<h3 id="创建-Tensor"><a href="#创建-Tensor" class="headerlink" title="创建 Tensor"></a>创建 Tensor</h3><h4 id="直接创建"><a href="#直接创建" class="headerlink" title="直接创建"></a>直接创建</h4><ul>
<li><p>torch.tensor(data, dtype=None, device=None,requires_grad=False)<br>  data - 可以是 list, tuple, numpy array, scalar 或其他类型</p>
<p>  dtype - 可以返回想要的tensor类型</p>
<p>  device - 可以指定返回的设备</p>
<p>  requires_grad - 可以指定是否进行记录图的操作，默认为 False</p>
<p>  需要注意的是，torch.tensor 总是会复制 data, 如果你想避免复制，可以使 torch.Tensor. detach()</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">2.1</span>, <span class="number">9</span>, <span class="number">1</span>]])</span><br><span class="line">tensor([[<span class="number">0.0000</span>, <span class="number">1.0000</span>, <span class="number">2.0000</span>],</span><br><span class="line">        [<span class="number">2.1000</span>, <span class="number">9.0000</span>, <span class="number">1.0000</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">4</span>)</span><br><span class="line">tensor(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([[<span class="number">0.11</span>, <span class="number">0.22</span>, <span class="number">0.33</span>]], dtype=torch.float32, device=torch.device(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">tensor([[<span class="number">0.1100</span>, <span class="number">0.2200</span>, <span class="number">0.3300</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([])</span><br><span class="line">tensor([])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="从-numpy-中获得数据"><a href="#从-numpy-中获得数据" class="headerlink" title="从 numpy 中获得数据"></a>从 numpy 中获得数据</h4><ul>
<li><p>torch.from_numpy(ndarry)</p>
<p>  注：生成返回的 tensor 会和 ndarry 共享数据，任何对 tensor 的操作都会影响到 ndarry ，反之亦然。</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = numpy.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.from_numpy(a)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.int32)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">0</span>] = -<span class="number">10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([-<span class="number">10</span>,   <span class="number">2</span>,   <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="创建特定-Tensor"><a href="#创建特定-Tensor" class="headerlink" title="创建特定 Tensor"></a>创建特定 Tensor</h4><ul>
<li><p>根据矩阵要求</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.eye(n, m=<span class="literal">None</span>, out=<span class="literal">None</span>, …)  <span class="comment"># 返回 2-D 的单位对角矩阵</span></span><br><span class="line">torch.empty(*sizes, out=<span class="literal">None</span>, …)  <span class="comment"># 返回被未初始化的数值填充，大小为 sizes 的 tensor</span></span><br><span class="line">torch.empty_like(<span class="built_in">input</span>, …)  <span class="comment"># 返回与 input 相同 size，并被未初始化的数值填充的 tensor</span></span><br></pre></td></tr></table></figure>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.eye(<span class="number">3</span>)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.empty((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">tensor([[<span class="number">0.0000e+00</span>, <span class="number">0.0000e+00</span>, <span class="number">2.8026e-45</span>],</span><br><span class="line">        [<span class="number">0.0000e+00</span>, <span class="number">1.4013e-45</span>, <span class="number">0.0000e+00</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.empty((<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.empty_like(a)</span><br><span class="line">tensor([[<span class="number">9.5510e-39</span>, <span class="number">9.4592e-39</span>, <span class="number">1.0745e-38</span>, <span class="number">6.2449e-39</span>],</span><br><span class="line">        [<span class="number">6.9796e-39</span>, <span class="number">8.4490e-39</span>, <span class="number">1.0561e-38</span>, <span class="number">9.1837e-39</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据数值要求</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros(*sizes, out=<span class="literal">None</span>, ..)  <span class="comment"># 返回大小为 sizes 的零矩阵 </span></span><br><span class="line"></span><br><span class="line">torch.zeros_like(<span class="built_in">input</span>, ..)  <span class="comment"># 返回与 input 相同 size 的零矩阵</span></span><br><span class="line"></span><br><span class="line">torch.ones(*sizes, out=<span class="literal">None</span>, ..)  <span class="comment"># 返回大小为 sizes 的单位矩阵</span></span><br><span class="line"></span><br><span class="line">torch.ones_like(<span class="built_in">input</span>, ..)  <span class="comment"># 返回与 input 相同 size 的单位矩阵</span></span><br><span class="line"></span><br><span class="line">torch.full(size, fill_value, …)  <span class="comment"># 返回大小为 sizes，单位值为 fill_value 的矩阵</span></span><br><span class="line"></span><br><span class="line">torch.full_like(<span class="built_in">input</span>, fill_value, …)  <span class="comment"># 返回与 input 相同 size，单位值为 fill_value 的矩阵</span></span><br><span class="line"></span><br><span class="line">torch.arange(start=<span class="number">0</span>, end, step=<span class="number">1</span>, …)  <span class="comment"># 返回从 start 到 end, 单位步长为 step 的 1-d tensor.</span></span><br><span class="line"></span><br><span class="line">torch.linspace(start, end, steps=<span class="number">100</span>, …)  <span class="comment"># 返回从 start 到 end, 间隔中的插值数目为 steps 的 1-d tensor</span></span><br><span class="line"></span><br><span class="line">torch.logspace(start, end, steps=<span class="number">100</span>, …)  <span class="comment"># 返回 1-d tensor ，从 10^start 到 10^end 的 steps 个对数间隔</span></span><br></pre></td></tr></table></figure>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.zeros((<span class="number">5</span>,<span class="number">4</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros_like((a))</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.ones((<span class="number">5</span>,<span class="number">4</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.ones_like((a))</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.full((<span class="number">2</span>, <span class="number">4</span>), <span class="number">3</span>)</span><br><span class="line">tensor([[<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.full((<span class="number">2</span>, <span class="number">3</span>), -<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.full_like(a, -<span class="number">6</span>)</span><br><span class="line">tensor([[-<span class="number">6.</span>, -<span class="number">6.</span>, -<span class="number">6.</span>],</span><br><span class="line">        [-<span class="number">6.</span>, -<span class="number">6.</span>, -<span class="number">6.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(-<span class="number">1</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">tensor([-<span class="number">1</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.linspace(-<span class="number">1</span>, <span class="number">510</span>, <span class="number">10</span>)</span><br><span class="line">tensor([ -<span class="number">1.0000</span>,  <span class="number">55.7778</span>, <span class="number">112.5556</span>, <span class="number">169.3333</span>, <span class="number">226.1111</span>, <span class="number">282.8889</span>, <span class="number">339.6667</span>,</span><br><span class="line">        <span class="number">396.4445</span>, <span class="number">453.2222</span>, <span class="number">510.0000</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.logspace(<span class="number">4</span>, <span class="number">12</span>, <span class="number">10</span>)</span><br><span class="line">tensor([<span class="number">1.0000e+04</span>, <span class="number">7.7426e+04</span>, <span class="number">5.9948e+05</span>, <span class="number">4.6416e+06</span>, <span class="number">3.5938e+07</span>, <span class="number">2.7826e+08</span>,</span><br><span class="line">        <span class="number">2.1544e+09</span>, <span class="number">1.6681e+10</span>, <span class="number">1.2915e+11</span>, <span class="number">1.0000e+12</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p>随机采样生成</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">torch.normal(mean, std, out=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">torch.rand(*size, *, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, …)  <span class="comment"># 返回 [0,1] 之间均匀分布的随机数值</span></span><br><span class="line"></span><br><span class="line">torch.rand_like(<span class="built_in">input</span>, dtype=<span class="literal">None</span>, …)  <span class="comment"># 返回与 input 相同 size 的 tensor, 填充均匀分布的随机数值</span></span><br><span class="line"></span><br><span class="line">torch.randint(low=<span class="number">0</span>, high, size,…)  <span class="comment"># 返回均匀分布的 [low,high] 之间的整数随机值</span></span><br><span class="line"></span><br><span class="line">torch.randint_like(<span class="built_in">input</span>, low=<span class="number">0</span>, high, dtype=<span class="literal">None</span>, …)</span><br><span class="line"></span><br><span class="line">torch.randn(*sizes, out=<span class="literal">None</span>, …)  <span class="comment"># 返回大小为 size，由均值为 0，方差为 1 的正态分布的随机数值</span></span><br><span class="line"></span><br><span class="line">torch.randn_like(<span class="built_in">input</span>, dtype=<span class="literal">None</span>, …)</span><br><span class="line"></span><br><span class="line">torch.randperm(n, out=<span class="literal">None</span>, dtype=torch.int64)  <span class="comment"># 返回 0 到 n-1 的数列的随机排列</span></span><br></pre></td></tr></table></figure>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.normal(mean=torch.arange(<span class="number">1.</span>, <span class="number">11.</span>), std=torch.arange(<span class="number">1</span>, <span class="number">0</span>, -<span class="number">0.1</span>))</span><br><span class="line">tensor([  <span class="number">1.0425</span>,   <span class="number">3.5672</span>,   <span class="number">2.7969</span>,   <span class="number">4.2925</span>,   <span class="number">4.7229</span>,   <span class="number">6.2134</span>,</span><br><span class="line">        <span class="number">8.0505</span>,   <span class="number">8.1408</span>,   <span class="number">9.0563</span>,  <span class="number">10.0566</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.normal(mean=<span class="number">0.5</span>, std=torch.arange(<span class="number">1.</span>, <span class="number">6.</span>))</span><br><span class="line">tensor([-<span class="number">1.2793</span>, -<span class="number">1.0732</span>, -<span class="number">2.0687</span>,  <span class="number">5.1177</span>, -<span class="number">1.2303</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.normal(<span class="number">2</span>, <span class="number">3</span>, size=(<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">tensor([[-<span class="number">1.3987</span>, -<span class="number">1.9544</span>,  <span class="number">3.6048</span>,  <span class="number">0.7909</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand((<span class="number">3</span>, <span class="number">4</span>), dtype=torch.float32)</span><br><span class="line">tensor([[<span class="number">0.7023</span>, <span class="number">0.4859</span>, <span class="number">0.3741</span>, <span class="number">0.3417</span>],</span><br><span class="line">        [<span class="number">0.4057</span>, <span class="number">0.1633</span>, <span class="number">0.8546</span>, <span class="number">0.9238</span>],</span><br><span class="line">        [<span class="number">0.1057</span>, <span class="number">0.9495</span>, <span class="number">0.6084</span>, <span class="number">0.9575</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand(<span class="number">4</span>, dtype=torch.float32)</span><br><span class="line">tensor([<span class="number">0.3772</span>, <span class="number">0.3302</span>, <span class="number">0.4888</span>, <span class="number">0.3880</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randn((<span class="number">4</span>, <span class="number">2</span>))</span><br><span class="line">tensor([[ <span class="number">0.5214</span>, -<span class="number">0.0752</span>],</span><br><span class="line">        [-<span class="number">0.6023</span>,  <span class="number">0.3023</span>],</span><br><span class="line">        [-<span class="number">0.8232</span>, -<span class="number">0.0204</span>],</span><br><span class="line">        [-<span class="number">1.5515</span>, -<span class="number">0.5482</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randn(<span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line">tensor([[-<span class="number">0.3684</span>,  <span class="number">0.6210</span>],</span><br><span class="line">        [ <span class="number">0.0618</span>, -<span class="number">0.9446</span>],</span><br><span class="line">        [-<span class="number">0.6084</span>,  <span class="number">1.6479</span>],</span><br><span class="line">        [-<span class="number">0.4306</span>, -<span class="number">0.4309</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randn(<span class="number">5</span>)</span><br><span class="line">tensor([-<span class="number">0.0086</span>, -<span class="number">0.6876</span>,  <span class="number">2.1599</span>, -<span class="number">1.8218</span>, -<span class="number">0.5487</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randperm(<span class="number">5</span>)</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="操作-Tensor"><a href="#操作-Tensor" class="headerlink" title="操作 Tensor"></a>操作 Tensor</h3><h4 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h4><ul>
<li><p>索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">torch.gather(<span class="built_in">input</span>, dim, index, out=<span class="literal">None</span>)：在指定维度上按照索引赋值输出 tensor。输入与输出大小一致</span><br><span class="line">out[i][j] = <span class="built_in">input</span>[index[i][j]][j]  <span class="comment"># if dim == 0</span></span><br><span class="line">out[i][j] = <span class="built_in">input</span>[i][index[i][j]]  <span class="comment"># if dim == 1</span></span><br><span class="line"></span><br><span class="line">torch.index_select(<span class="built_in">input</span>, dim, index, out=<span class="literal">None</span>)：选出维度的一些 <span class="built_in">slice</span> 组合成新的 tensor。指定维度的大小与 index 大小一致</span><br><span class="line"></span><br><span class="line">torch.masked_select(<span class="built_in">input</span>, mask, out=<span class="literal">None</span>)：按照 mask 输出一个一维的 tensor</span><br><span class="line"></span><br><span class="line">torch.take(<span class="built_in">input</span>, indices)：将输入看成 1D tensor，按照索引得到输出。输出大小与 index 大小一致</span><br><span class="line"></span><br><span class="line">torch.nonzero(<span class="built_in">input</span>, out=<span class="literal">None</span>)：输出非 <span class="number">0</span> 元素的坐标</span><br><span class="line"></span><br><span class="line">torch.where(condition, x, y)：按照条件从 x 和 y 中选出满足条件的元素组成新的 tensor</span><br><span class="line"><span class="keyword">if</span> condition_&#123;i&#125; out_&#123;i&#125; = x_&#123;i&#125; <span class="keyword">else</span> out_&#123;i&#125; = y_&#123;i&#125; </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([[<span class="number">0</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.gather(t, <span class="number">0</span>, torch.tensor([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>]]))</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.gather(t, <span class="number">1</span>, torch.tensor([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>]]))</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[<span class="number">0.6347</span>, <span class="number">0.7947</span>, <span class="number">0.8334</span>, <span class="number">0.8211</span>],</span><br><span class="line">        [<span class="number">0.6180</span>, <span class="number">0.7563</span>, <span class="number">0.2590</span>, <span class="number">0.6282</span>],</span><br><span class="line">        [<span class="number">0.6226</span>, <span class="number">0.3971</span>, <span class="number">0.1406</span>, <span class="number">0.9478</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>index = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.index_select(x, <span class="number">0</span>, index)</span><br><span class="line">tensor([[<span class="number">0.6347</span>, <span class="number">0.7947</span>, <span class="number">0.8334</span>, <span class="number">0.8211</span>],</span><br><span class="line">        [<span class="number">0.6226</span>, <span class="number">0.3971</span>, <span class="number">0.1406</span>, <span class="number">0.9478</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.index_select(x, <span class="number">1</span>, index)</span><br><span class="line">tensor([[<span class="number">0.6347</span>, <span class="number">0.8334</span>],</span><br><span class="line">        [<span class="number">0.6180</span>, <span class="number">0.2590</span>],</span><br><span class="line">        [<span class="number">0.6226</span>, <span class="number">0.1406</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[<span class="number">0.3344</span>, <span class="number">0.4833</span>, <span class="number">0.1678</span>, <span class="number">0.5293</span>],</span><br><span class="line">        [<span class="number">0.4132</span>, <span class="number">0.1088</span>, <span class="number">0.6891</span>, <span class="number">0.3533</span>],</span><br><span class="line">        [<span class="number">0.7509</span>, <span class="number">0.0925</span>, <span class="number">0.1463</span>, <span class="number">0.0815</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask = x.le(<span class="number">0.5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]], dtype=torch.uint8)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.masked_select(x, mask)</span><br><span class="line">tensor([<span class="number">0.3344</span>, <span class="number">0.4833</span>, <span class="number">0.1678</span>, <span class="number">0.4132</span>, <span class="number">0.1088</span>, <span class="number">0.3533</span>, <span class="number">0.0925</span>, <span class="number">0.1463</span>, <span class="number">0.0815</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[<span class="number">0.9591</span>, <span class="number">0.9192</span>, <span class="number">0.8893</span>, <span class="number">0.5640</span>],</span><br><span class="line">        [<span class="number">0.6016</span>, <span class="number">0.4091</span>, <span class="number">0.1549</span>, <span class="number">0.2757</span>],</span><br><span class="line">        [<span class="number">0.4370</span>, <span class="number">0.2869</span>, <span class="number">0.3915</span>, <span class="number">0.9391</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>index = torch.tensor([<span class="number">0</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.take(x, index)</span><br><span class="line">tensor([<span class="number">0.9591</span>, <span class="number">0.1549</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[<span class="number">0.3374</span>, <span class="number">0.4527</span>, <span class="number">0.5701</span>],</span><br><span class="line">        [<span class="number">0.0378</span>, <span class="number">0.8197</span>, <span class="number">0.6043</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y</span><br><span class="line">tensor([[<span class="number">0.9215</span>, <span class="number">0.1330</span>, <span class="number">0.0794</span>],</span><br><span class="line">        [<span class="number">0.6500</span>, <span class="number">0.9862</span>, <span class="number">0.3328</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.where(x &gt; <span class="number">0.5</span>, x, y)</span><br><span class="line">tensor([[<span class="number">0.9215</span>, <span class="number">0.1330</span>, <span class="number">0.5701</span>],</span><br><span class="line">        [<span class="number">0.6500</span>, <span class="number">0.8197</span>, <span class="number">0.6043</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>分块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.chunk(tensor, chunks, dim=<span class="number">0</span>)：按照某个维度平均分块（最后一个可能小于平均值）</span><br><span class="line"></span><br><span class="line">torch.split(tensor, split_size_or_sections, dim=<span class="number">0</span>)：按照某个维度依照第二个参数给出的 <span class="built_in">list</span> 或者 <span class="built_in">int</span> 进行分割 tensor</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[<span class="number">0.1225</span>, <span class="number">0.3073</span>, <span class="number">0.9378</span>, <span class="number">0.0690</span>],</span><br><span class="line">        [<span class="number">0.1011</span>, <span class="number">0.5872</span>, <span class="number">0.8539</span>, <span class="number">0.5885</span>],</span><br><span class="line">        [<span class="number">0.4126</span>, <span class="number">0.7177</span>, <span class="number">0.5187</span>, <span class="number">0.0285</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.chunk(x, <span class="number">2</span>)</span><br><span class="line">(tensor([[<span class="number">0.1225</span>, <span class="number">0.3073</span>, <span class="number">0.9378</span>, <span class="number">0.0690</span>],</span><br><span class="line">        [<span class="number">0.1011</span>, <span class="number">0.5872</span>, <span class="number">0.8539</span>, <span class="number">0.5885</span>]]), tensor([[<span class="number">0.4126</span>, <span class="number">0.7177</span>, <span class="number">0.5187</span>, <span class="number">0.0285</span>]]))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.rand(<span class="number">5</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[<span class="number">0.4513</span>, <span class="number">0.6542</span>],</span><br><span class="line">        [<span class="number">0.0945</span>, <span class="number">0.3659</span>],</span><br><span class="line">        [<span class="number">0.7489</span>, <span class="number">0.0688</span>],</span><br><span class="line">        [<span class="number">0.4222</span>, <span class="number">0.8877</span>],</span><br><span class="line">        [<span class="number">0.1566</span>, <span class="number">0.2200</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.split(x, <span class="number">2</span>)</span><br><span class="line">(tensor([[<span class="number">0.4513</span>, <span class="number">0.6542</span>],</span><br><span class="line">        [<span class="number">0.0945</span>, <span class="number">0.3659</span>]]), tensor([[<span class="number">0.7489</span>, <span class="number">0.0688</span>],</span><br><span class="line">        [<span class="number">0.4222</span>, <span class="number">0.8877</span>]]), tensor([[<span class="number">0.1566</span>, <span class="number">0.2200</span>]]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.split(x, [<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">(tensor([[<span class="number">0.4513</span>, <span class="number">0.6542</span>]]), tensor([[<span class="number">0.0945</span>, <span class="number">0.3659</span>],</span><br><span class="line">        [<span class="number">0.7489</span>, <span class="number">0.0688</span>],</span><br><span class="line">        [<span class="number">0.4222</span>, <span class="number">0.8877</span>],</span><br><span class="line">        [<span class="number">0.1566</span>, <span class="number">0.2200</span>]]))</span><br></pre></td></tr></table></figure>
</li>
<li><p>组合</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.cat(seq, dim=<span class="number">0</span>, out=<span class="literal">None</span>)：按照已经存在的维度进行 concatenate</span><br><span class="line"></span><br><span class="line">torch.stack(seq, dim=<span class="number">0</span>, out=<span class="literal">None</span>)：按照新的维度进行 concatenate</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.rand(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[<span class="number">0.8151</span>, <span class="number">0.7717</span>],</span><br><span class="line">        [<span class="number">0.4125</span>, <span class="number">0.2779</span>],</span><br><span class="line">        [<span class="number">0.4898</span>, <span class="number">0.6496</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.rand(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y</span><br><span class="line">tensor([[<span class="number">0.9355</span>, <span class="number">0.6364</span>],</span><br><span class="line">        [<span class="number">0.9170</span>, <span class="number">0.6370</span>],</span><br><span class="line">        [<span class="number">0.9311</span>, <span class="number">0.1291</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, y), <span class="number">0</span>)</span><br><span class="line">tensor([[<span class="number">0.8151</span>, <span class="number">0.7717</span>],</span><br><span class="line">        [<span class="number">0.4125</span>, <span class="number">0.2779</span>],</span><br><span class="line">        [<span class="number">0.4898</span>, <span class="number">0.6496</span>],</span><br><span class="line">        [<span class="number">0.9355</span>, <span class="number">0.6364</span>],</span><br><span class="line">        [<span class="number">0.9170</span>, <span class="number">0.6370</span>],</span><br><span class="line">        [<span class="number">0.9311</span>, <span class="number">0.1291</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x), <span class="number">1</span>)</span><br><span class="line">tensor([[<span class="number">0.8151</span>, <span class="number">0.7717</span>, <span class="number">0.8151</span>, <span class="number">0.7717</span>],</span><br><span class="line">        [<span class="number">0.4125</span>, <span class="number">0.2779</span>, <span class="number">0.4125</span>, <span class="number">0.2779</span>],</span><br><span class="line">        [<span class="number">0.4898</span>, <span class="number">0.6496</span>, <span class="number">0.4898</span>, <span class="number">0.6496</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.stack((x, x), <span class="number">1</span>)</span><br><span class="line">tensor([[[<span class="number">0.8151</span>, <span class="number">0.7717</span>],</span><br><span class="line">         [<span class="number">0.8151</span>, <span class="number">0.7717</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.4125</span>, <span class="number">0.2779</span>],</span><br><span class="line">         [<span class="number">0.4125</span>, <span class="number">0.2779</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.4898</span>, <span class="number">0.6496</span>],</span><br><span class="line">         [<span class="number">0.4898</span>, <span class="number">0.6496</span>]]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>变形</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.reshape(<span class="built_in">input</span>, shape)</span><br><span class="line"></span><br><span class="line">torch.t(<span class="built_in">input</span>)： 只针对 2D tensor 转置</span><br><span class="line"></span><br><span class="line">torch.transpose(<span class="built_in">input</span>, dim0, dim1)：交换两个维度</span><br><span class="line"></span><br><span class="line">torch.squeeze(<span class="built_in">input</span>, dim=<span class="literal">None</span>, out=<span class="literal">None</span>)：去除那些维度大小为 <span class="number">1</span> 的维度</span><br><span class="line"></span><br><span class="line">torch.unbind(tensor, dim=<span class="number">0</span>)：去除某个维度</span><br><span class="line"></span><br><span class="line">torch.unsqueeze(<span class="built_in">input</span>, dim, out=<span class="literal">None</span>)：在指定位置添加维度</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randint(<span class="number">1</span>, <span class="number">13</span>, (<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">3</span>,  <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">        [<span class="number">12</span>, <span class="number">11</span>,  <span class="number">2</span>],</span><br><span class="line">        [<span class="number">11</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">        [<span class="number">10</span>, <span class="number">11</span>,  <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.reshape(x, (<span class="number">6</span>, <span class="number">2</span>))</span><br><span class="line">tensor([[ <span class="number">3</span>,  <span class="number">5</span>],</span><br><span class="line">        [ <span class="number">6</span>, <span class="number">12</span>],</span><br><span class="line">        [<span class="number">11</span>,  <span class="number">2</span>],</span><br><span class="line">        [<span class="number">11</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">11</span>,  <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.t(x)</span><br><span class="line">tensor([[ <span class="number">3</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">10</span>],</span><br><span class="line">        [ <span class="number">5</span>, <span class="number">11</span>,  <span class="number">3</span>, <span class="number">11</span>],</span><br><span class="line">        [ <span class="number">6</span>,  <span class="number">2</span>,  <span class="number">4</span>,  <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.transpose(x, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">3</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">10</span>],</span><br><span class="line">        [ <span class="number">5</span>, <span class="number">11</span>,  <span class="number">3</span>, <span class="number">11</span>],</span><br><span class="line">        [ <span class="number">6</span>,  <span class="number">2</span>,  <span class="number">4</span>,  <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.zeros(<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.squeeze(x)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randint(<span class="number">1</span>, <span class="number">20</span>, (<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">4</span>, <span class="number">10</span>,  <span class="number">1</span>],</span><br><span class="line">        [<span class="number">19</span>, <span class="number">13</span>, <span class="number">19</span>],</span><br><span class="line">        [ <span class="number">8</span>,  <span class="number">4</span>, <span class="number">16</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unbind(x)</span><br><span class="line">(tensor([ <span class="number">4</span>, <span class="number">10</span>,  <span class="number">1</span>]), tensor([<span class="number">19</span>, <span class="number">13</span>, <span class="number">19</span>]), tensor([ <span class="number">8</span>,  <span class="number">4</span>, <span class="number">16</span>]), tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">3</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unbind(x, <span class="number">1</span>)</span><br><span class="line">(tensor([ <span class="number">4</span>, <span class="number">19</span>,  <span class="number">8</span>,  <span class="number">4</span>]), tensor([<span class="number">10</span>, <span class="number">13</span>,  <span class="number">4</span>,  <span class="number">5</span>]), tensor([ <span class="number">1</span>, <span class="number">19</span>, <span class="number">16</span>,  <span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unsqueeze(x, <span class="number">0</span>)</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unsqueeze(x, <span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a>数学运算</h4><ul>
<li><p>点对点操作</p>
<ul>
<li>三角函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.acos(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.asin(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.atan(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.atan2(<span class="built_in">input</span>, input2, out=<span class="literal">None</span>) </span><br><span class="line">torch.cos(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.cosh(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.sin(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.sinh(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.tan(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line">torch.tanh(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>加减乘除</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.add(<span class="built_in">input</span>, value, out=<span class="literal">None</span>)</span><br><span class="line">torch.add(<span class="built_in">input</span>, value=<span class="number">1</span>, other, out=<span class="literal">None</span>)  <span class="comment"># out = input + value * other</span></span><br><span class="line">torch.addcdiv(tensor, value=<span class="number">1</span>, tensor1, tensor2, out=<span class="literal">None</span>)  <span class="comment"># out = tensor + value * tensor1 / tensor2</span></span><br><span class="line">torch.addcmul(tensor, value=<span class="number">1</span>, tensor1, tensor2, out=<span class="literal">None</span>)  <span class="comment"># out = tensor + value * tensor1 * tensor2</span></span><br><span class="line">torch.div(<span class="built_in">input</span>, value, out=<span class="literal">None</span>)</span><br><span class="line">torch.div(<span class="built_in">input</span>, other, out=<span class="literal">None</span>)</span><br><span class="line">torch.mul(<span class="built_in">input</span>, value, out=<span class="literal">None</span>)</span><br><span class="line">torch.mul(<span class="built_in">input</span>, other, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>对数运算</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.log(<span class="built_in">input</span>, out=<span class="literal">None</span>)  <span class="comment"># y_i = log_e(x_i)</span></span><br><span class="line">torch.log1p(<span class="built_in">input</span>, out=<span class="literal">None</span>)  <span class="comment"># y_i = log_e(x_i + 1)</span></span><br><span class="line">torch.log2(<span class="built_in">input</span>, out=<span class="literal">None</span>)   <span class="comment"># y_i = log_2(x_i)</span></span><br><span class="line">torch.log10(<span class="built_in">input</span>,out=<span class="literal">None</span>)  <span class="comment"># y_i = log_10(x_i)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>幂函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">pow</span>(<span class="built_in">input</span>, exponent, out=<span class="literal">None</span>)  <span class="comment"># y_i = input ^ (exponent)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>指数函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(tensor, out=<span class="literal">None</span>)  <span class="comment"># y_i = e ^ (x_i)</span></span><br><span class="line">torch.expm1(tensor, out=<span class="literal">None</span>)  <span class="comment"># y_i = e ^ (x_i) - 1</span></span><br></pre></td></tr></table></figure>
<ul>
<li>截断函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.ceil(<span class="built_in">input</span>, out=<span class="literal">None</span>)   <span class="comment"># 返回向正方向取得最小整数</span></span><br><span class="line">torch.floor(<span class="built_in">input</span>, out=<span class="literal">None</span>)  <span class="comment"># 返回向负方向取得最大整数</span></span><br><span class="line"></span><br><span class="line">torch.<span class="built_in">round</span>(<span class="built_in">input</span>, out=<span class="literal">None</span>)  <span class="comment"># 返回相邻最近的整数，四舍五入</span></span><br><span class="line"></span><br><span class="line">torch.trunc(<span class="built_in">input</span>, out=<span class="literal">None</span>)  <span class="comment"># 返回整数部分数值</span></span><br><span class="line">torch.frac(tensor, out=<span class="literal">None</span>)  <span class="comment"># 返回小数部分数值</span></span><br><span class="line"></span><br><span class="line">torch.fmod(<span class="built_in">input</span>, divisor, out=<span class="literal">None</span>)  <span class="comment"># 返回 input/divisor 的余数</span></span><br><span class="line">torch.remainder(<span class="built_in">input</span>, divisor, out=<span class="literal">None</span>)  <span class="comment"># 同上</span></span><br></pre></td></tr></table></figure>
<ul>
<li>其他</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">torch.erf(tensor， out=<span class="literal">None</span>)</span><br><span class="line"> </span><br><span class="line">torch.erfinv(tensor, out=<span class="literal">None</span>)</span><br><span class="line"> </span><br><span class="line">torch.sigmoid(<span class="built_in">input</span>, out=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">torch.clamp(<span class="built_in">input</span>, <span class="built_in">min</span>, <span class="built_in">max</span> out=<span class="literal">None</span>)  <span class="comment"># 返回 input &lt; min，则返回 min，input &gt; max，则返回 max，其余返回 input</span></span><br><span class="line"></span><br><span class="line">torch.neg(<span class="built_in">input</span>, out=<span class="literal">None</span>)  <span class="comment"># out_i = -1 * (input)</span></span><br><span class="line"></span><br><span class="line">torch.reciprocal(<span class="built_in">input</span>, out=<span class="literal">None</span>)  <span class="comment"># out_i = 1 / input_i</span></span><br><span class="line"></span><br><span class="line">torch.sqrt(<span class="built_in">input</span>, out=<span class="literal">None</span>)  <span class="comment"># out_i = sqrt(input_i)</span></span><br><span class="line">torch.rsqrt(<span class="built_in">input</span>, out=<span class="literal">None</span>)  <span class="comment"># out_i = 1 / (sqrt(input_i))</span></span><br><span class="line"></span><br><span class="line">torch.sign(<span class="built_in">input</span>, out=<span class="literal">None</span>)  <span class="comment"># out_i = sin(input_i)  大于 0 为 1，小于 0 为 -1</span></span><br><span class="line"></span><br><span class="line">torch.lerp(start, end, weight, out=<span class="literal">None</span>)  <span class="comment"># out = start + weight * (end - start)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>降维操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">torch.argmax(<span class="built_in">input</span>, dim=<span class="literal">None</span>, keepdim=<span class="literal">False</span>)  <span class="comment"># 返回最大值排序的索引值</span></span><br><span class="line">torch.argmin(<span class="built_in">input</span>, dim=<span class="literal">None</span>, keepdim=<span class="literal">False</span>)  <span class="comment"># 返回最小值排序的索引值</span></span><br><span class="line"></span><br><span class="line">torch.cumprod(<span class="built_in">input</span>, dim, out=<span class="literal">None</span>)  <span class="comment"># y_i = x_1 * x_2 * x_3 * … * x_i</span></span><br><span class="line">torch.cumsum(<span class="built_in">input</span>, dim, out=<span class="literal">None</span>)  <span class="comment"># y_i = x_1 + x_2 + … + x_i</span></span><br><span class="line"></span><br><span class="line">torch.dist(<span class="built_in">input</span>, out, p=<span class="number">2</span>)  <span class="comment"># 返回 input 和 out 的 p 式距离</span></span><br><span class="line">torch.mean()  <span class="comment"># 返回平均值</span></span><br><span class="line">torch.<span class="built_in">sum</span>() <span class="comment"># 返回总和</span></span><br><span class="line">torch.median(<span class="built_in">input</span>) <span class="comment"># 返回中间值</span></span><br><span class="line">torch.mode(<span class="built_in">input</span>) <span class="comment"># 返回众数值</span></span><br><span class="line">torch.unique(<span class="built_in">input</span>, <span class="built_in">sorted</span>=<span class="literal">False</span>)  <span class="comment"># 返回 1-D 的唯一的 tensor，每个数值返回一次</span></span><br><span class="line">torch.std()  <span class="comment"># 返回标准差</span></span><br><span class="line">torch.var()  <span class="comment"># 返回方差</span></span><br><span class="line"></span><br><span class="line">torch.norm(<span class="built_in">input</span>, p=<span class="number">2</span>)  <span class="comment"># 返回 p-norm 的范式</span></span><br><span class="line">torch.prod(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>)  <span class="comment"># 返回指定维度每一行的乘积</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>对比操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">torch.eq(<span class="built_in">input</span>, other, out=<span class="literal">None</span>)  <span class="comment"># 按成员进行等式操作，相同返回 1</span></span><br><span class="line">torch.equal(tensor1, tensor2)  <span class="comment"># 如果 tensor1 和 tensor2 有相同的 size 和 elements，则为 true</span></span><br><span class="line"></span><br><span class="line">torch.ge(<span class="built_in">input</span>, other, out=<span class="literal">None</span>)  <span class="comment"># input &gt;= other</span></span><br><span class="line">torch.gt(<span class="built_in">input</span>, other, out=<span class="literal">None</span>)  <span class="comment"># input &gt; other</span></span><br><span class="line">torch.le(<span class="built_in">input</span>, other, out=<span class="literal">None</span>)  <span class="comment"># input =&lt; other</span></span><br><span class="line">torch.lt(<span class="built_in">input</span>, other, out=<span class="literal">None</span>)  <span class="comment"># input &lt; other</span></span><br><span class="line">torch.ne(<span class="built_in">input</span>, other, out=<span class="literal">None</span>)  <span class="comment"># input != other</span></span><br><span class="line"></span><br><span class="line">torch.<span class="built_in">max</span>()  <span class="comment"># 返回最大值</span></span><br><span class="line">torch.<span class="built_in">min</span>()  <span class="comment"># 返回最小值</span></span><br><span class="line">torch.isnan(tensor)  <span class="comment"># 判断是否为 ’nan’</span></span><br><span class="line">torch.sort(<span class="built_in">input</span>, dim=<span class="literal">None</span>, descending=<span class="literal">False</span>, out=<span class="literal">None</span>)  <span class="comment"># 对目标 input 进行排序</span></span><br><span class="line">torch.topk(<span class="built_in">input</span>, k, dim=<span class="literal">None</span>, largest=<span class="literal">True</span>, <span class="built_in">sorted</span>=<span class="literal">True</span>, out=<span class="literal">None</span>)  <span class="comment"># 沿着指定维度返回最大 k 个数值及其索引值</span></span><br><span class="line">torch.kthvalue(<span class="built_in">input</span>, k, dim=<span class="literal">None</span>, deepdim=<span class="literal">False</span>, out=<span class="literal">None</span>)  <span class="comment"># 沿着指定维度返回最小 k 个数值及其索引值</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>频谱操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.fft(<span class="built_in">input</span>, signal_ndim, normalized=<span class="literal">False</span>)</span><br><span class="line">torch.ifft(<span class="built_in">input</span>, signal_ndim, normalized=<span class="literal">False</span>)</span><br><span class="line">torch.rfft(<span class="built_in">input</span>, signal_ndim, normalized=<span class="literal">False</span>, onesided=<span class="literal">True</span>)</span><br><span class="line">torch.irfft(<span class="built_in">input</span>, signal_ndim, normalized=<span class="literal">False</span>, onesided=<span class="literal">True</span>)</span><br><span class="line">torch.stft(signa, frame_length, hop, …)</span><br></pre></td></tr></table></figure>
</li>
<li><p>其他</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">torch.cross(<span class="built_in">input</span>, other, dim=-<span class="number">1</span>, out=<span class="literal">None</span>)  <span class="comment"># 叉乘(外积)</span></span><br><span class="line"></span><br><span class="line">torch.dot(tensor1, tensor2)  <span class="comment"># 返回 tensor1 和 tensor2 的点乘</span></span><br><span class="line"></span><br><span class="line">torch.mm(mat1, mat2, out=<span class="literal">None</span>)  <span class="comment"># 返回矩阵 mat1 和 mat2 的乘积</span></span><br><span class="line"></span><br><span class="line">torch.eig(a, eigenvectors=<span class="literal">False</span>, out=<span class="literal">None</span>)  <span class="comment"># 返回矩阵 a 的特征值/特征向量 </span></span><br><span class="line"></span><br><span class="line">torch.det(A)  <span class="comment"># 返回矩阵 A 的行列式</span></span><br><span class="line"></span><br><span class="line">torch.trace(<span class="built_in">input</span>)  <span class="comment"># 返回 2-d 矩阵的迹(对对角元素求和)</span></span><br><span class="line"></span><br><span class="line">torch.diag(<span class="built_in">input</span>, diagonal=<span class="number">0</span>, out=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">torch.histc(<span class="built_in">input</span>, bins=<span class="number">100</span>, <span class="built_in">min</span>=<span class="number">0</span>, <span class="built_in">max</span>=<span class="number">0</span>, out=<span class="literal">None</span>)  <span class="comment"># 计算 input 的直方图</span></span><br><span class="line"></span><br><span class="line">torch.tril(<span class="built_in">input</span>, diagonal=<span class="number">0</span>, out=<span class="literal">None</span>)  <span class="comment"># 返回矩阵的下三角矩阵，其他为 0</span></span><br><span class="line"></span><br><span class="line">torch.triu(<span class="built_in">input</span>, diagonal=<span class="number">0</span>, out=<span class="literal">None</span>)  <span class="comment"># 返回矩阵的上三角矩阵，其他为 0</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>tips</strong>：通过一些内置函数，可以实现对 tensor 的精度, 类型，print 打印参数等进行设置。</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.set_default_dtype(d)  <span class="comment"># 对 torch.tensor() 设置默认的浮点类型</span></span><br><span class="line">torch.set_default_tensor_type()  <span class="comment"># 同上，对torch.tensor()设置默认的 tensor 类型</span></span><br><span class="line"></span><br><span class="line">torch.get_default_dtype()  <span class="comment"># 获得当前默认的浮点类型 torch.dtype</span></span><br><span class="line"></span><br><span class="line">torch.set_printoptions(precision=<span class="literal">None</span>, threshold=<span class="literal">None</span>, edgeitems=<span class="literal">None</span>, linewidth=<span class="literal">None</span>, profile=<span class="literal">None</span>） <span class="comment"># 设置 printing 的打印参数</span></span><br></pre></td></tr></table></figure>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1.2</span>, <span class="number">3</span>]).dtype  <span class="comment"># initial default for floating point is torch.float32</span></span><br><span class="line">torch.float32</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_default_dtype(torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1.2</span>, <span class="number">3</span>]).dtype  <span class="comment"># a new floating point tensor</span></span><br><span class="line">torch.float64</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_default_tensor_type(torch.DoubleTensor)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1.2</span>, <span class="number">3</span>]).dtype  <span class="comment"># a new floating point tensor</span></span><br><span class="line">torch.float64</span><br></pre></td></tr></table></figure>

      </section>

      
      
        <nav class="article-nav">
          
            <div class="article-nav-item layout-padding">
  <article class="card-container article-nav-card content-padding--primary soft-size--large soft-style--box">
    
      <div class="card-cover" background-image-lazy data-img="https://cdn.magdeleine.co/wp-content/uploads/2022/06/49963511106_fd6151abc3_o-1400x932.jpg"></div>
    
    <div class="card-text">
      
        <a href="/2021/11/24/DeepLearning/0.3%20%E8%87%AA%E5%8A%A8%E6%B1%82%E6%A2%AF%E5%BA%A6/" itemprop="url">
          <h2 class="card-text--title text-ellipsis">自动求梯度</h2>
        </a>
      
      <div class="card-text--row">Newer</div>
    </div>
  </article>
</div>
          
          
            <div class="article-nav-item layout-padding">
  <article class="card-container article-nav-card content-padding--primary soft-size--large soft-style--box">
    
      <div class="card-cover" background-image-lazy data-img="https://cdn.magdeleine.co/wp-content/uploads/2016/07/photo-1428342319217-5fdaf6d7898e-1400x652.jpeg"></div>
    
    <div class="card-text">
      
        <a href="/2021/11/23/DeepLearning/0.1%20Pytorch%20%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" itemprop="url">
          <h2 class="card-text--title text-ellipsis">Pytorch 环境配置</h2>
        </a>
      
      <div class="card-text--row">Older</div>
    </div>
  </article>
</div>
          
        </nav>
      

      <section class="page-message-container layout-padding">
        


  
  
    <div class="valine-container comments-container content-padding--primary soft-size--large soft-style--box">
      <div id="valine_thread" class="valine-thread"></div>
    </div>
    <script type="text/javascript" src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script type="text/javascript" src='//unpkg.com/valine/dist/Valine.min.js'></script>
    <script type="text/javascript">
      new Valine({
        el: "#valine_thread",
        appId: "kijfJ6KM8wj7sqyTIbyRTPoV-gzGzoHsz",
        appKey: "flq2Xa3mOpDb24YvPGLXya9w",
        avatar: "mm",
        placeholder: "随便说点什么叭～",
        notify: true,
        visitor: true,
        pageSize: 10,
      });
    </script>
  

  
  


      </section>
    </div>
    <div class="widget-info">
      <section class="widget-author widget-item layout-margin content-padding--primary soft-size--large soft-style--box">
  <div class="widget-body">
    
      <img src="https://avatars.githubusercontent.com/u/54944075?v=4" class="soft-size--round soft-style--box" alt="怪兽少女">
    
    
      <h2>怪兽少女</h2>
    
    
      <p>可爱无敌，秃头不慌！！！</p>
    

    <div class="count-box">
      <div class="count-box--item">
        <svg class="icon icon-article" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M240.51564747 647.74217627h196.07203239c16.59071043 0 30.16492806-13.57421762 30.16492805-30.16492806V165.10332731c0-33.18142087-30.16492806-60.32985613-60.32985612-60.32985611H245.04038668C225.43318342 104.7734712 210.35071939 119.85593522 210.35071939 139.46313845V617.57724821c0 16.59071043 13.57421762 30.16492806 30.16492808 30.16492806z m663.62841731-452.47392089v482.63884894c0 33.18142087-27.14843525 60.32985613-60.32985612 60.32985613H180.18579134c-33.18142087 0-60.32985613-27.14843525-60.32985612-60.32985613V195.26825538c-49.77213131 0-90.49478418 40.72265287-90.49478417 90.49478417v452.4739209c0 49.77213131 40.72265287 90.49478418 90.49478417 90.49478417h286.56681657c16.59071043 0 30.16492806 13.57421762 30.16492807 30.16492807s13.57421762 30.16492806 30.16492805 30.16492806h90.49478418c16.59071043 0 30.16492806-13.57421762 30.16492805-30.16492806s13.57421762-30.16492806 30.16492807-30.16492807h286.56681657c49.77213131 0 90.49478418-40.72265287 90.49478417-90.49478417V285.76303955c0-49.77213131-40.72265287-90.49478418-90.49478417-90.49478417zM587.41232014 647.74217627h191.54729318c19.60720323 0 34.68966726-15.08246403 34.68966729-34.68966727V134.93839925c0-16.59071043-13.57421762-30.16492806-30.16492808-30.16492805H617.57724821c-30.16492806 0-60.32985613 27.14843525-60.32985612 60.32985611v452.4739209c0 16.59071043 13.57421762 30.16492806 30.16492805 30.16492806z" fill="currentColor"></path>
</svg>
        <span>11</span>
      </div>
      <div class="count-box--item">
        <svg class="icon icon-categories" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M900.3614811 257.09082106h-339.81629553l-67.96326003-101.9448889c-19.41807444-29.12711113-48.54518557-43.69066667-82.52681443-43.69066667H123.6385189c-53.39970333 0-97.09036999 43.69066667-97.09037113 97.09036999v582.54222222c0 53.39970333 43.69066667 97.09036999 97.09037113 97.09037002h776.7229622c53.39970333 0 97.09036999-43.69066667 97.09037113-97.09037002V354.18119104c0-53.39970333-43.69066667-97.09036999-97.09037113-97.09036998z m-97.09036999 242.72592554H220.72888889c-24.27259221 0-48.54518557-24.27259221-48.54518556-48.54518556s24.27259221-48.54518557 48.54518556-48.54518444h582.54222222c24.27259221 0 48.54518557 24.27259221 48.54518556 48.54518444s-24.27259221 48.54518557-48.54518556 48.54518556z" fill="currentColor"></path>
</svg>
        3
      </div>
      <div class="count-box--item">
        <svg class="icon icon-tags" viewBox="0 0 1098 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M283.42180005 272q0-28.38857157-20.09142843-48.48000001t-48.47999998-20.09142842-48.48000002 20.09142842-20.09142846 48.48000001 20.09142846 48.48 48.48000002 20.09142843 48.47999998-20.09142843 20.09142843-48.48zM855.0332285 580.57142843q0 28.38857157-19.81714313 48.2057147l-263.03999997 263.58857157q-20.9142853 19.81714313-48.75428534 19.81714312-28.38857157 0-48.20571468-19.81714312l-383.04-383.58857157q-20.36571468-19.81714313-34.55999999-54.10285688t-14.19428534-62.6742853l0-222.85714313q0-27.84000002 20.36571469-48.20571469t48.2057147-20.36571466l222.85714313 0q28.38857157 0 62.6742853 14.19428529t54.65142842 34.55999999l383.04000001 382.49142843q19.81714313 20.9142853 19.81714314 48.75428532zM1060.74751475 580.57142843q0 28.38857157-19.81714313 48.2057147l-263.04 263.58857157q-20.9142853 19.81714313-48.75428531 19.81714312-19.26857155 0-31.61142843-7.47428531t-28.38857159-24.13714314l251.79428534-251.7942853q19.81714313-19.81714313 19.81714308-48.20571469 0-27.84000002-19.81714308-48.75428531l-383.04000001-382.49142845q-20.36571468-20.36571468-54.65142842-34.55999999t-62.67428532-14.19428534l120 0q28.38857157 0 62.67428532 14.19428534t54.65142842 34.55999999l383.03999998 382.49142845q19.81714313 20.9142853 19.81714314 48.75428531z" fill="currentColor"></path>
</svg>
        9
      </div>
    </div>
  </div>
</section>

      
<section class="widget-toc widget-item layout-margin content-padding--primary soft-size--large soft-style--box">
  <div class="widget-title">
    <svg class="icon icon-toc" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M134.50666666 767.46666668H460.8c27.73333333 0 50.24000001 22.50666668 50.24000001 50.23999999v50.13333333c0 27.73333333-22.50666668 50.24000001-50.24000001 50.24000001H134.50666666c-27.73333333 0-50.24000001-22.50666668-50.23999999-50.24000001v-50.13333333c0.10666668-27.73333333 22.50666668-50.24000001 50.24000001-50.24000001zM84.37333332 541.65333333h326.18666669c27.73333333 0 50.24000001 22.39999999 50.23999999 50.13333334v50.24000001c0 27.73333333-22.50666668 50.24000001-50.24000002 50.23999999H84.37333332c-27.73333333 0-50.24000001-22.50666668-50.23999999-50.23999999v-50.24000001c0-27.73333333 22.50666668-50.13333334 50.24000001-50.13333334zM134.50666666 315.83999999H460.8c27.73333333 0 50.24000001 22.50666668 50.24000001 50.24000001v50.24000001c0 27.73333333-22.50666668 50.13333334-50.24000001 50.13333333H134.50666666c-27.73333333 0-50.24000001-22.39999999-50.23999999-50.13333333v-50.24000001c0.10666668-27.84000001 22.50666668-50.24000001 50.24000001-50.23999999zM209.81333332 89.91999999h326.18666671c27.73333333 0 50.24000001 22.39999999 50.23999997 50.13333335v50.23999999c0 27.73333333-22.50666668 50.24000001-50.24000001 50.24000001H209.81333332c-27.73333333 0-50.24000001-22.50666668-50.23999999-50.24000001v-50.24000001c0-27.73333333 22.50666668-50.13333334 50.24000001-50.13333333zM692.05333333 623.36l274.66666669 176.00000002c23.36000001 14.93333333 30.08 45.97333334 15.14666666 69.33333332L954.77333334 910.93333333c-14.93333333 23.25333334-45.97333334 30.08-69.33333335 15.14666667l-274.66666666-176c-23.36000001-14.93333333-30.08-45.97333334-15.14666667-69.33333333l27.09333334-42.24000001c14.93333333-23.36000001 46.08000001-30.08 69.33333333-15.14666666z" fill="currentColor"></path>
</svg>
    <span>TOC</span>
  </div>
  <div class="widget-body">
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-number">1.</span> <span class="toc-text">数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%9E%E6%80%A7"><span class="toc-number">1.1.</span> <span class="toc-text">属性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-Tensor"><span class="toc-number">1.2.</span> <span class="toc-text">创建 Tensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C-Tensor"><span class="toc-number">1.3.</span> <span class="toc-text">操作 Tensor</span></a></li></ol></li></ol>
  </div>
</section>


      

      <section class="widget-categorys widget-item layout-margin content-padding--primary soft-size--large soft-style--box">
  <div class="widget-title">
    <svg class="icon icon-categories" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M900.3614811 257.09082106h-339.81629553l-67.96326003-101.9448889c-19.41807444-29.12711113-48.54518557-43.69066667-82.52681443-43.69066667H123.6385189c-53.39970333 0-97.09036999 43.69066667-97.09037113 97.09036999v582.54222222c0 53.39970333 43.69066667 97.09036999 97.09037113 97.09037002h776.7229622c53.39970333 0 97.09036999-43.69066667 97.09037113-97.09037002V354.18119104c0-53.39970333-43.69066667-97.09036999-97.09037113-97.09036998z m-97.09036999 242.72592554H220.72888889c-24.27259221 0-48.54518557-24.27259221-48.54518556-48.54518556s24.27259221-48.54518557 48.54518556-48.54518444h582.54222222c24.27259221 0 48.54518557 24.27259221 48.54518556 48.54518444s-24.27259221 48.54518557-48.54518556 48.54518556z" fill="currentColor"></path>
</svg>
    <span>CATEGORYS</span>
  </div>
  <div class="widget-body">
    <ul class="categorys-list">
      
        <li class="categorys-list-item">
          <a href="/categories/DeepLearning/">
            DeepLearning (5)
          </a>
        </li>
      
        <li class="categorys-list-item">
          <a href="/categories/P4/">
            P4 (4)
          </a>
        </li>
      
        <li class="categorys-list-item">
          <a href="/categories/Psychology/">
            Psychology (1)
          </a>
        </li>
      
    </ul>
  </div>
</section>

      <section class="widget-tags widget-item  layout-margin content-padding--primary soft-size--large soft-style--box">
  <div class="widget-title">
    <svg class="icon icon-tags" viewBox="0 0 1098 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M283.42180005 272q0-28.38857157-20.09142843-48.48000001t-48.47999998-20.09142842-48.48000002 20.09142842-20.09142846 48.48000001 20.09142846 48.48 48.48000002 20.09142843 48.47999998-20.09142843 20.09142843-48.48zM855.0332285 580.57142843q0 28.38857157-19.81714313 48.2057147l-263.03999997 263.58857157q-20.9142853 19.81714313-48.75428534 19.81714312-28.38857157 0-48.20571468-19.81714312l-383.04-383.58857157q-20.36571468-19.81714313-34.55999999-54.10285688t-14.19428534-62.6742853l0-222.85714313q0-27.84000002 20.36571469-48.20571469t48.2057147-20.36571466l222.85714313 0q28.38857157 0 62.6742853 14.19428529t54.65142842 34.55999999l383.04000001 382.49142843q19.81714313 20.9142853 19.81714314 48.75428532zM1060.74751475 580.57142843q0 28.38857157-19.81714313 48.2057147l-263.04 263.58857157q-20.9142853 19.81714313-48.75428531 19.81714312-19.26857155 0-31.61142843-7.47428531t-28.38857159-24.13714314l251.79428534-251.7942853q19.81714313-19.81714313 19.81714308-48.20571469 0-27.84000002-19.81714308-48.75428531l-383.04000001-382.49142845q-20.36571468-20.36571468-54.65142842-34.55999999t-62.67428532-14.19428534l120 0q28.38857157 0 62.67428532 14.19428534t54.65142842 34.55999999l383.03999998 382.49142845q19.81714313 20.9142853 19.81714314 48.75428531z" fill="currentColor"></path>
</svg>
    <span>TAGS</span>
  </div>
  <div class="widget-body">
    <div class="tags-cloud">
      <a href="/tags/DeepLearning/" style="font-size: 20px;" class="tags-cloud-10">DeepLearning</a> <a href="/tags/Life/" style="font-size: 10px;" class="tags-cloud-0">Life</a> <a href="/tags/LinearRegression/" style="font-size: 10px;" class="tags-cloud-0">LinearRegression</a> <a href="/tags/P4/" style="font-size: 15px;" class="tags-cloud-5">P4</a> <a href="/tags/P4-Tutorials/" style="font-size: 10px;" class="tags-cloud-0">P4-Tutorials</a> <a href="/tags/P4-Utils/" style="font-size: 10px;" class="tags-cloud-0">P4-Utils</a> <a href="/tags/Psychology/" style="font-size: 10px;" class="tags-cloud-0">Psychology</a> <a href="/tags/Pytorch/" style="font-size: 15px;" class="tags-cloud-5">Pytorch</a> <a href="/tags/Ubuntu/" style="font-size: 10px;" class="tags-cloud-0">Ubuntu</a>
    </div>
  </div>
</section>
    </div>
  </article>
</div>

    <!-- footer container -->
<footer id="footer" class="footer">
  <div class="footer-container">
    
    <div class="social-icons">
      
        
      
        
      
        
      
        
          <a href="https://github.com/king-wk/" class="soft-size--primary soft-style--box" target="_blank" rel="noopener noreferrer">
            <svg class="icon icon-github" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M64.6 512c0 195.6 125.4 361.9 300.1 422.9 23.5 5.9 19.9-10.8 19.9-22.2v-77.6c-135.8 15.9-141.3-74-150.5-89-18.5-31.5-61.9-39.5-49-54.5 31-15.9 62.5 4 98.9 58 26.4 39.1 77.9 32.5 104.1 26 5.7-23.5 17.9-44.5 34.7-60.9-140.7-25.2-199.4-111.1-199.4-213.3 0-49.5 16.4-95.1 48.4-131.8-20.4-60.6 1.9-112.4 4.9-120.1 58.2-5.2 118.5 41.6 123.3 45.3 33.1-8.9 70.8-13.7 112.9-13.7 42.4 0 80.3 4.9 113.5 13.9 11.3-8.6 67.3-48.8 121.4-43.9 2.9 7.7 24.7 58.3 5.5 118.1 32.5 36.8 49 82.8 49 132.4 0 102.3-59 188.3-200.2 213.2 23.5 23.3 38.1 55.5 38.1 91.1v112.7c0.8 9 0 17.9 15.1 17.9C832.7 877 960.4 709.4 960.4 512.1c0-247.5-200.6-447.9-447.9-447.9C265 64.1 64.6 264.5 64.6 512z"></path>
</svg>
          </a>
        
      
    </div>
     
    <p>&copy; 2023 <a href="/" target="_blank">double 静</a></p>

    

    <p>Powered by <a href="https://hexo.io" target="_blank" rel="noopener noreferrer">Hexo</a> Theme - <a href="https://github.com/miiiku/flex-block" target="_blank" rel="noopener noreferrer author">flex-block</a></p>

    <p>
      <a href="javascript:;" id="theme-light">🌞 浅色</a>
      <a href="javascript:;" id="theme-dark">🌛 深色</a>
      <a href="javascript:;" id="theme-auto">🤖️ 自动</a>
    </p>
  </div>
</footer>
  </div>

  <div class="back-to-top-fixed soft-size--round soft-style--box">
    <svg class="icon icon-back-to-top" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
      <path d="M725.333333 426.666667c-12.8 0-21.333333-4.266667-29.866667-12.8l-213.333333-213.333333c-17.066667-17.066667-17.066667-42.666667 0-59.733333s42.666667-17.066667 59.733333 0l213.333333 213.333333c17.066667 17.066667 17.066667 42.666667 0 59.733333C746.666667 422.4 738.133333 426.666667 725.333333 426.666667z"></path>
      <path d="M298.666667 426.666667c-12.8 0-21.333333-4.266667-29.866667-12.8-17.066667-17.066667-17.066667-42.666667 0-59.733333l213.333333-213.333333c17.066667-17.066667 42.666667-17.066667 59.733333 0s17.066667 42.666667 0 59.733333l-213.333333 213.333333C320 422.4 311.466667 426.666667 298.666667 426.666667z"></path>
      <path d="M512 896c-25.6 0-42.666667-17.066667-42.666667-42.666667L469.333333 170.666667c0-25.6 17.066667-42.666667 42.666667-42.666667s42.666667 17.066667 42.666667 42.666667l0 682.666667C554.666667 878.933333 537.6 896 512 896z"></path>
    </svg>
  </div>

  
  <!-- aplayer -->


<!-- dplayer -->




  


  


  




<script src="/js/script.js"></script>


  
  <!-- 尾部用户自定义相关内容 -->
</body>
</html>